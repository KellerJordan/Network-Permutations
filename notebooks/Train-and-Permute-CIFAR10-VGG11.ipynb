{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9021242c",
   "metadata": {},
   "source": [
    "# Train-and-Permute-CIFAR10-VGG11\n",
    "\n",
    "This notebook executes the following:\n",
    "\n",
    "1. We train two normalization-free, 2x width VGG11 networks, calling them model0 and model1.\n",
    "2. We compute a set of permutations which align the neurons of model1 with those of model0. We use a correlation-based method which goes back to https://arxiv.org/abs/1511.07543.\n",
    "3. We evaluate the accuracy of the interpolation between model0 and the neuron-permuted version of model1. It's not very good -- accuracy drops from ~90% at the \"endpoint\" networks to ~70% at the midpoint.\n",
    "4. We investigate the cause of this drop. We find that the later hidden units of the interpolated network have \"collapsed variance\".\n",
    "5. We introduce a method of correcting for this phenomenon: For each unit in the interpolated network, we rescale it such that its statistics are similar to those of the parent/endpoint networks.\n",
    "6. We re-evaluate the interpolated network after this correction has been applied, and find that it gets 87% test accuracy. So the gap / \"barrier\" has shrunk from 90-70 = 20% to only 3%. The barrier in terms of test loss is also below 0.1 after correction.\n",
    "\n",
    "We also note that the idea of resetting the BatchNorm statistics of interpolated networks goes at least back to https://arxiv.org/abs/1803.05407. The method we introduce here can be thought of as a generalization of BatchNorm-reset to networks which don't use BatchNorm!\n",
    "\n",
    "For some comparable numbers we can look at the VGG curve of Figure 4 in Git Re-Basin (https://arxiv.org/abs/2209.04836). The authors use a permutation-only method with LayerNorm-based networks, and report a test loss barrier of ~0.25 for 2x-width VGG-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0db779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1a55e",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c477d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a directory to save VGG checkpoints\n",
    "os.makedirs('./vgg', exist_ok=True)\n",
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), 'vgg/%s.pt' % i)\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load('vgg/%s.pt' % i)\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabbc157",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## CIFAR-10 dataloaders -- we use FFCV because it's fast\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "\n",
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "\n",
    "## fast FFCV data loaders\n",
    "device = 'cuda:0' \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "pre_p = [SimpleRGBImageDecoder()]\n",
    "post_p = [\n",
    "    ToTensor(),\n",
    "    ToDevice(device, non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float16),\n",
    "    T.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "]\n",
    "aug_p = [\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomTranslate(padding=4),\n",
    "]\n",
    "\n",
    "\n",
    "train_aug_loader = Loader(f'/tmp/cifar_train.beton',\n",
    "                      batch_size=500,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.RANDOM,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+aug_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "train_noaug_loader = Loader(f'/tmp/cifar_train.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})\n",
    "test_loader = Loader(f'/tmp/cifar_test.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e22780",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation functions\n",
    "# evaluates accuracy\n",
    "def evaluate(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct\n",
    "\n",
    "# evaluates acc and loss\n",
    "def evaluate2(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "            total += len(labels)\n",
    "            loss = F.cross_entropy(outputs, labels.cuda())\n",
    "            losses.append(loss.item())\n",
    "    return correct / total, np.array(losses).mean()\n",
    "\n",
    "def full_eval(model):\n",
    "    tr_acc, tr_loss = evaluate2(model, loader=train_noaug_loader)\n",
    "    te_acc, te_loss = evaluate2(model, loader=test_loader)\n",
    "    return '%.2f, %.3f, %.2f, %.3f' % (100*tr_acc, tr_loss, 100*te_acc, te_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cdbf3",
   "metadata": {},
   "source": [
    "## Train and save two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b6c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kuangliu/pytorch-cifar/blob/master/models/vgg.py\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, w=1, bn=False):\n",
    "        super(VGG, self).__init__()\n",
    "        self.vgg_name = vgg_name\n",
    "        self.bn = bn\n",
    "        self.w = w\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(self.w*512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(in_channels if in_channels == 3 else self.w*in_channels,\n",
    "                                     self.w*x, kernel_size=3, padding=1))\n",
    "                if self.bn:\n",
    "                    layers.append(nn.BatchNorm2d(self.w*x))\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9527af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(w=1):\n",
    "    model = VGG('VGG11', w=w).cuda()\n",
    "    optimizer = SGD(model.parameters(), lr=0.08, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    EPOCHS = 100\n",
    "    ne_iters = len(train_aug_loader)\n",
    "    lr_schedule = np.interp(np.arange(1+EPOCHS*ne_iters), [0, 5*ne_iters, EPOCHS*ne_iters], [0, 1, 0])\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_schedule.__getitem__)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_aug_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                outputs = model(inputs.cuda())\n",
    "                loss = loss_fn(outputs, labels.cuda())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            losses.append(loss.item())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137befdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [03:50<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [03:48<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9016\n"
     ]
    }
   ],
   "source": [
    "w = 2\n",
    "model = train_model(w)\n",
    "print(evaluate(model))\n",
    "save_model(model, 'vgg11x%d_v1b' % w)\n",
    "\n",
    "model = train_model(w)\n",
    "print(evaluate(model))\n",
    "save_model(model, 'vgg11x%d_v2b' % w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cacbb",
   "metadata": {},
   "source": [
    "### matching code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363e68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given two networks net0, net1 which each output a feature map of shape NxCxWxH\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the outputs of the two networks\n",
    "def run_corr_matrix(net0, net1, epochs=1, norm=True, loader=train_aug_loader):\n",
    "    n = epochs*len(loader)\n",
    "    mean0 = mean1 = std0 = std1 = None\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for _ in range(epochs):\n",
    "            for i, (images, _) in enumerate(tqdm(loader)):\n",
    "                img_t = images.float().cuda()\n",
    "                out0 = net0(img_t)\n",
    "                out0 = out0.reshape(out0.shape[0], out0.shape[1], -1).permute(0, 2, 1)\n",
    "                out0 = out0.reshape(-1, out0.shape[2]).double()\n",
    "\n",
    "                out1 = net1(img_t)\n",
    "                out1 = out1.reshape(out1.shape[0], out1.shape[1], -1).permute(0, 2, 1)\n",
    "                out1 = out1.reshape(-1, out1.shape[2]).double()\n",
    "\n",
    "                mean0_b = out0.mean(dim=0)\n",
    "                mean1_b = out1.mean(dim=0)\n",
    "                std0_b = out0.std(dim=0)\n",
    "                std1_b = out1.std(dim=0)\n",
    "                outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "\n",
    "                if i == 0:\n",
    "                    mean0 = torch.zeros_like(mean0_b)\n",
    "                    mean1 = torch.zeros_like(mean1_b)\n",
    "                    std0 = torch.zeros_like(std0_b)\n",
    "                    std1 = torch.zeros_like(std1_b)\n",
    "                    outer = torch.zeros_like(outer_b)\n",
    "                mean0 += mean0_b / n\n",
    "                mean1 += mean1_b / n\n",
    "                std0 += std0_b / n\n",
    "                std1 += std1_b / n\n",
    "                outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    if norm:\n",
    "        corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "        return corr\n",
    "    else:\n",
    "        return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1aae9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifies the weight matrices of a convolution and batchnorm\n",
    "# layer given a permutation of the output channels\n",
    "def permute_output(perm_map, conv, bn):\n",
    "    pre_weights = [\n",
    "        conv.weight,\n",
    "    ]\n",
    "    if conv.bias is not None:\n",
    "        pre_weights.append(conv.bias)\n",
    "    if bn is not None:\n",
    "        pre_weights.extend([\n",
    "            bn.weight,\n",
    "            bn.bias,\n",
    "            bn.running_mean,\n",
    "            bn.running_var,\n",
    "        ])\n",
    "    for w in pre_weights:\n",
    "        w.data = w[perm_map]\n",
    "\n",
    "# modifies the weight matrix of a layer for a given permutation of the input channels\n",
    "# works for both conv2d and linear\n",
    "def permute_input(perm_map, layer):\n",
    "    w = layer.weight\n",
    "    w.data = w[:, perm_map]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8b84a",
   "metadata": {},
   "source": [
    "# Find neuron-permutation for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0db278e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9045, 9016)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = VGG('VGG11', w=w).cuda()\n",
    "model1 = VGG('VGG11', w=w).cuda()\n",
    "load_model(model0, 'vgg11x%d_v1b' % w)\n",
    "load_model(model1, 'vgg11x%d_v2b' % w)\n",
    "\n",
    "evaluate(model0), evaluate(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb660be",
   "metadata": {},
   "source": [
    "## Permuting neurons in model1 to match model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b99f10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 98.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 91.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 66.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 55.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 49.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 45.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 38.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 41.65it/s]\n"
     ]
    }
   ],
   "source": [
    "def subnet(model, n_layers):\n",
    "    return model.features[:n_layers]\n",
    "\n",
    "feats1 = model1.features\n",
    "\n",
    "n = len(feats1)\n",
    "for i in range(n):\n",
    "    layer = feats1[i]\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        # get permutation and permute output of conv and maybe bn\n",
    "        if isinstance(feats1[i+1], nn.BatchNorm2d):\n",
    "            assert isinstance(feats1[i+2], nn.ReLU)\n",
    "            perm_map = get_layer_perm(subnet(model0, i+3), subnet(model1, i+3))\n",
    "            permute_output(perm_map, feats1[i], feats1[i+1])\n",
    "        else:\n",
    "            assert isinstance(feats1[i+1], nn.ReLU)\n",
    "            perm_map = get_layer_perm(subnet(model0, i+2), subnet(model1, i+2))\n",
    "            permute_output(perm_map, feats1[i], None)\n",
    "        # look for succeeding layer to permute input\n",
    "        next_layer = None\n",
    "        for j in range(i+1, n):\n",
    "            if isinstance(feats1[j], nn.Conv2d):\n",
    "                next_layer = feats1[j]\n",
    "                break\n",
    "        if next_layer is None:\n",
    "            next_layer = model1.classifier\n",
    "        permute_input(perm_map, next_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4b4d80",
   "metadata": {},
   "source": [
    "### Save permuted weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3112ee07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9015\n"
     ]
    }
   ],
   "source": [
    "# ensure accuracy didn't change\n",
    "# (it may be slightly different due to non-associativity of floating point arithmetic)\n",
    "print(evaluate(model1))\n",
    "save_model(model1, 'vgg11x%d_v2b_perm1b' % w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc494077",
   "metadata": {},
   "source": [
    "## Evaluate the interpolated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7ae6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(model, alpha, key0, key1):\n",
    "    sd0 = torch.load('vgg/%s.pt' % key0)\n",
    "    sd1 = torch.load('vgg/%s.pt' % key1)\n",
    "    sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "                for k in sd0.keys()}\n",
    "    model.load_state_dict(sd_alpha)\n",
    "\n",
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_aug_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71d15852",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.59, 0.815, 73.12, 0.904\n"
     ]
    }
   ],
   "source": [
    "model_a = VGG('VGG11', w=w).cuda()\n",
    "mix_weights(model_a, 0.5, 'vgg11x%d_v1b' % w, 'vgg11x%d_v2b_perm1b' % w)\n",
    "print(full_eval(model_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06da3f",
   "metadata": {},
   "source": [
    "## Diagnosing the problem: measuring unitwise variances\n",
    "\n",
    "The interpolated network got 70% train accuracy -- not very good, much worse than the endpoints which get over 90%. Why does this happen?\n",
    "\n",
    "(We find that) in later layers of the interpolated network, the variance of hidden units has \"collapsed\", relative to the variance of the same hidden units in the original/parent/endpoint networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "860f4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, _ = next(iter(train_aug_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "871893bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# returns (vv0, vva, vv1)\n",
    "# where vv0 is a list of layerwise variances (as defined in paper)\n",
    "# for model0 (alpha=0.0)\n",
    "# and vva is the same for the midpoint (alpha=0.5)\n",
    "# and vv1 for model1 (alpha=1.0)\n",
    "def get_layerwise_vars(k0, k1):\n",
    "    vvs = []\n",
    "    for alpha in [0, 0.5, 1]:\n",
    "        mix_weights(model_a, alpha, k0, k1)\n",
    "        vv = []\n",
    "        for i in [1, 4, 7, 9, 12, 14, 17, 19]:\n",
    "#         for i in [2, 5, 8, 10, 13, 15, 18, 20]:\n",
    "            subnet = model_a.features[:i]\n",
    "            with torch.no_grad(), autocast():\n",
    "                out = subnet(inputs)\n",
    "            out = out.permute(1, 0, 2, 3).reshape(out.size(1), -1)\n",
    "            avg_var = out.var(1).mean()\n",
    "            vv.append(avg_var.item())\n",
    "        vvs.append(np.array(vv))\n",
    "    vv0, vva, vv1 = vvs\n",
    "    return vv0, vva, vv1\n",
    "\n",
    "# returns a list of ratios between the variance of\n",
    "# the weight-interpolation midpoint and the averaged variances\n",
    "# of the two endpoints.\n",
    "def get_layerwise_ratios(k0, k1):\n",
    "    vv0, vva, vv1 = get_layerwise_vars(k0, k1)\n",
    "    vv00 = (vv0+vv1)/2\n",
    "    rr = vva/vv0\n",
    "    return rr\n",
    "\n",
    "k0 = 'vgg11x%d_nobn_v1' % w\n",
    "k1 = 'vgg11x%d_nobn_v2' % w\n",
    "rr1 = get_layerwise_ratios(k0, k1)\n",
    "\n",
    "k0 = 'vgg11x%d_nobn_v1' % w\n",
    "k1 = 'vgg11x%d_nobn_v2_perm1' % w\n",
    "rr2 = get_layerwise_ratios(k0, k1)\n",
    "\n",
    "plt.plot(rr1, label='Without neuron matching')\n",
    "plt.plot(rr2, label='With neuron matching')\n",
    "# plt.yscale('log')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Layer index')\n",
    "plt.title('VGG11 layerwise variance ratios')\n",
    "plt.ylabel('$\\\\dfrac{\\\\sigma_{0.5}}{(\\\\sigma_0 + \\\\sigma_1)/2}$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89396da3",
   "metadata": {},
   "source": [
    "## Solving the problem: Activation Renormalization\n",
    "\n",
    "We introduce a rescaling of each neuron's preactivation in the interpolated network, such that the statistics of each neuron are set to be similar to those of the endpoint networks. Practically, this is accomplished by the temporary introduction of PyTorch nn.BatchNorm2d layers, which are then \"fused\" back into the preceding convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ff057c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetConv(nn.Module):\n",
    "    def __init__(self, conv):\n",
    "        super().__init__()\n",
    "        self.h = h = conv.out_channels\n",
    "        self.conv = conv\n",
    "        self.bn = nn.BatchNorm2d(h)\n",
    "        self.rescale = False\n",
    "        \n",
    "    def set_stats(self, goal_mean, goal_var):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        goal_std = (goal_var + 1e-5).sqrt()\n",
    "        self.bn.weight.data = goal_std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.rescale:\n",
    "            x = self.bn(x)\n",
    "        else:\n",
    "            self.bn(x)\n",
    "        return x\n",
    "\n",
    "def make_tracked_net(net, w):\n",
    "    net1 = VGG('VGG11', w=w).cuda()\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    feats1 = net1.features\n",
    "    for i, layer in enumerate(feats1):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            feats1[i] = ResetConv(layer)\n",
    "    return net1.cuda().eval()\n",
    "\n",
    "def fuse_conv_bn(conv, bn):\n",
    "    fused = torch.nn.Conv2d(\n",
    "        conv.in_channels,\n",
    "        conv.out_channels,\n",
    "        kernel_size=conv.kernel_size,\n",
    "        stride=conv.stride,\n",
    "        padding=conv.padding,\n",
    "        bias=True,\n",
    "    )\n",
    "\n",
    "    # setting weights\n",
    "    w_conv = conv.weight.clone()\n",
    "    bn_std = (bn.eps + bn.running_var).sqrt()\n",
    "    gamma = bn.weight / bn_std\n",
    "    fused.weight.data = (w_conv * gamma.reshape(-1, 1, 1, 1))\n",
    "\n",
    "    # setting bias\n",
    "    b_conv = conv.bias if conv.bias is not None else torch.zeros_like(bn.bias)\n",
    "    beta = bn.bias + gamma * (-bn.running_mean + b_conv)\n",
    "    fused.bias.data = beta\n",
    "    \n",
    "    return fused\n",
    "\n",
    "def fuse_tracked_net(net, w):\n",
    "    net1 = VGG('VGG11', w=w).cuda()\n",
    "    feats1 = net1.features\n",
    "    for i, layer in enumerate(net.features):\n",
    "        if isinstance(layer, ResetConv):\n",
    "            conv = fuse_conv_bn(layer.conv, layer.bn)\n",
    "            feats1[i].load_state_dict(conv.state_dict())\n",
    "    net1.classifier.load_state_dict(net.classifier.state_dict())\n",
    "    return net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b8f5931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc, train_loss, test_acc, test_loss...\n",
      "(α=0.0) 99.98, 0.002, 90.43, 0.451\n",
      "(α=0.5) 73.66, 0.744, 69.57, 0.858\n",
      "(α=1.0) 99.99, 0.002, 90.30, 0.439\n"
     ]
    }
   ],
   "source": [
    "model0 = VGG('VGG11', w).cuda()\n",
    "model_a = VGG('VGG11', w).cuda()\n",
    "model1 = VGG('VGG11', w).cuda()\n",
    "\n",
    "k0 = 'vgg11x%d_nobn_v1' % w\n",
    "k1 = 'vgg11x%d_nobn_v2_perm1' % w\n",
    "mix_weights(model0, 0.0, k0, k1)\n",
    "mix_weights(model_a, 0.5, k0, k1)\n",
    "mix_weights(model1, 1.0, k0, k1)\n",
    "\n",
    "print('train_acc, train_loss, test_acc, test_loss...')\n",
    "print('(α=0.0)', full_eval(model0))\n",
    "print('(α=0.5)', full_eval(model_a))\n",
    "print('(α=1.0)', full_eval(model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80f95d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the statistics of every hidden unit in the endpoint networks\n",
    "## this is done practically using PyTorch BatchNorm2d layers.\n",
    "wrap0 = make_tracked_net(model0, w=w)\n",
    "wrap1 = make_tracked_net(model1, w=w)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)\n",
    "\n",
    "wrap_a = make_tracked_net(model_a, w=w)\n",
    "## set the goal mean/std in added bns of interpolated network, and turn batch renormalization on\n",
    "for m0, m_a, m1 in zip(wrap0.modules(), wrap_a.modules(), wrap1.modules()):\n",
    "    if not isinstance(m0, ResetConv):\n",
    "        continue\n",
    "    # get goal statistics -- interpolate the mean and std of parent networks\n",
    "    mu0 = m0.bn.running_mean\n",
    "    mu1 = m1.bn.running_mean\n",
    "    goal_mean = (mu0 + mu1)/2\n",
    "    var0 = m0.bn.running_var\n",
    "    var1 = m1.bn.running_var\n",
    "    goal_var = ((var0.sqrt() + var1.sqrt())/2).square()\n",
    "    # set these in the interpolated bn controller\n",
    "    m_a.set_stats(goal_mean, goal_var)\n",
    "    # turn rescaling on\n",
    "    m_a.rescale = True\n",
    "    \n",
    "# reset the tracked mean/var and fuse rescalings back into conv layers \n",
    "reset_bn_stats(wrap_a)\n",
    "# fuse the rescaling+shift coefficients back into conv layers\n",
    "model_b = fuse_tracked_net(wrap_a, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a39f02b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0.5 corrected) 94.82, 0.156, 87.08, 0.539\n"
     ]
    }
   ],
   "source": [
    "print('(α=0.5 corrected)', full_eval(model_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b675c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e2d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
