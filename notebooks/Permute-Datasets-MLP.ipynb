{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e5ba46",
   "metadata": {},
   "source": [
    "# Train-and-Permute-MNIST-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0db779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e738d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1a55e",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c477d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), '%s.pt' % i)\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load('%s.pt' % i)\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "61156810",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_MEAN = [33.3184]*3\n",
    "MNIST_STD = [78.5675]*3\n",
    "\n",
    "## fast FFCV data loaders\n",
    "device = 'cuda:0' \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "pre_p = [SimpleRGBImageDecoder()]\n",
    "post_p = [\n",
    "    ToTensor(),\n",
    "    ToDevice(device, non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float16),\n",
    "    T.Normalize(MNIST_MEAN, MNIST_STD),\n",
    "]\n",
    "aug_p = [\n",
    "    RandomTranslate(padding=4),\n",
    "]\n",
    "\n",
    "\n",
    "train_loader = Loader(f'/tmp/mnist_train.beton',\n",
    "                      batch_size=2000,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.RANDOM,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+aug_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "train_noaug_loader = Loader(f'/tmp/mnist_train.beton',\n",
    "                      batch_size=2000,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.SEQUENTIAL,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "test_loader = Loader(f'/tmp/mnist_test.beton',\n",
    "                     batch_size=2000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d8451d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_MEAN = [72.9404]*3\n",
    "MNIST_STD = [90.0212]*3\n",
    "\n",
    "## fast FFCV data loaders\n",
    "device = 'cuda:0' \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "pre_p = [SimpleRGBImageDecoder()]\n",
    "post_p = [\n",
    "    ToTensor(),\n",
    "    ToDevice(device, non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float16),\n",
    "    T.Normalize(MNIST_MEAN, MNIST_STD),\n",
    "]\n",
    "aug_p = [\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomTranslate(padding=4),\n",
    "]\n",
    "\n",
    "\n",
    "train_loader = Loader(f'/tmp/fashionmnist_train.beton',\n",
    "                      batch_size=2000,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.RANDOM,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+aug_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "train_noaug_loader = Loader(f'/tmp/fashionmnist_train.beton',\n",
    "                      batch_size=2000,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.SEQUENTIAL,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "test_loader = Loader(f'/tmp/fashionmnist_test.beton',\n",
    "                     batch_size=2000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c7c88610",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVHN_MEAN = [111.6095, 113.1604, 120.5646]\n",
    "SVHN_STD = [50.4977, 51.2590, 50.2442]\n",
    "\n",
    "## fast FFCV data loaders\n",
    "device = 'cuda:0' \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "pre_p = [SimpleRGBImageDecoder()]\n",
    "post_p = [\n",
    "    ToTensor(),\n",
    "    ToDevice(device, non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float16),\n",
    "    T.Normalize(SVHN_MEAN, SVHN_STD),\n",
    "]\n",
    "aug_p = [\n",
    "    RandomTranslate(padding=4),\n",
    "]\n",
    "\n",
    "\n",
    "train_loader = Loader(f'/tmp/svhn_train.beton',\n",
    "                      batch_size=2000,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.RANDOM,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+aug_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "train_noaug_loader = Loader(f'/tmp/svhn_train.beton',\n",
    "                      batch_size=2000,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.SEQUENTIAL,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "test_loader = Loader(f'/tmp/svhn_test.beton',\n",
    "                     batch_size=2000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2d992864",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "\n",
    "## fast FFCV data loaders\n",
    "device = 'cuda:0' \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "pre_p = [SimpleRGBImageDecoder()]\n",
    "post_p = [\n",
    "    ToTensor(),\n",
    "    ToDevice(device, non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float16),\n",
    "    T.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "]\n",
    "aug_p = [\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomTranslate(padding=4),\n",
    "]\n",
    "\n",
    "\n",
    "train_loader = Loader(f'/tmp/cifar_train.beton',\n",
    "                      batch_size=2000,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.RANDOM,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+aug_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "train_noaug_loader = Loader(f'/tmp/cifar_train.beton',\n",
    "                      batch_size=2000,\n",
    "                      num_workers=8,\n",
    "                      order=OrderOption.SEQUENTIAL,\n",
    "                      drop_last=True,\n",
    "                      pipelines={'image': pre_p+post_p,\n",
    "                                 'label': label_pipeline})\n",
    "test_loader = Loader(f'/tmp/cifar_test.beton',\n",
    "                     batch_size=2000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "31e22780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct\n",
    "\n",
    "# evaluates acc and loss\n",
    "def evaluate2(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "            total += len(labels)\n",
    "            loss = F.cross_entropy(outputs, labels.cuda())\n",
    "            losses.append(loss.item())\n",
    "    return correct / total, np.array(losses).mean()\n",
    "\n",
    "def full_eval1(model):\n",
    "    tr_acc, tr_loss = evaluate2(model, loader=train_noaug_loader)\n",
    "    te_acc, te_loss = evaluate2(model, loader=test_loader)\n",
    "    return '%.2f, %.3f, %.2f, %.3f' % (100*tr_acc, tr_loss, 100*te_acc, te_loss)\n",
    "\n",
    "def full_eval(model):\n",
    "    tr_acc, tr_loss = evaluate2(model, loader=train_noaug_loader)\n",
    "    te_acc, te_loss = evaluate2(model, loader=test_loader)\n",
    "    return (100*tr_acc, tr_loss, 100*te_acc, te_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "eb3707bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, h=128, layers=3, dset='mnist'):\n",
    "        super().__init__()\n",
    "        self.dset = dset\n",
    "        self.grayscale = (dset in ['mnist', 'fashionmnist'])\n",
    "        dim1 = 28*28 if self.grayscale else 3*32*32\n",
    "        self.fc1 = nn.Linear(dim1, h)\n",
    "        mid_layers = []\n",
    "        for _ in range(layers):\n",
    "            mid_layers.extend([nn.Linear(h, h), nn.ReLU()])\n",
    "        self.layers = nn.Sequential(*mid_layers)\n",
    "        self.fc2 = nn.Linear(h, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.grayscale:\n",
    "            if x.size(1) == 3:\n",
    "                x = x.mean(1, keepdim=True)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.layers(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e46b7b",
   "metadata": {},
   "source": [
    "### matching code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "363e68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given two networks net0, net1 which each output a feature map of shape NxCxWxH\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the outputs of the two networks\n",
    "def run_corr_matrix(net0, net1):\n",
    "    n = len(train_loader)\n",
    "    mean0 = mean1 = std0 = std1 = None\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for i, (images, _) in enumerate(tqdm(train_loader)):\n",
    "            img_t = images.float().cuda()\n",
    "            out0 = net0(img_t)\n",
    "            out0 = out0.reshape(out0.shape[0], out0.shape[1], -1).permute(0, 2, 1)\n",
    "            out0 = out0.reshape(-1, out0.shape[2]).double()\n",
    "\n",
    "            out1 = net1(img_t)\n",
    "            out1 = out1.reshape(out1.shape[0], out1.shape[1], -1).permute(0, 2, 1)\n",
    "            out1 = out1.reshape(-1, out1.shape[2]).double()\n",
    "\n",
    "            mean0_b = out0.mean(dim=0)\n",
    "            mean1_b = out1.mean(dim=0)\n",
    "            std0_b = out0.std(dim=0)\n",
    "            std1_b = out1.std(dim=0)\n",
    "            outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "\n",
    "            if i == 0:\n",
    "                mean0 = torch.zeros_like(mean0_b)\n",
    "                mean1 = torch.zeros_like(mean1_b)\n",
    "                std0 = torch.zeros_like(std0_b)\n",
    "                std1 = torch.zeros_like(std1_b)\n",
    "                outer = torch.zeros_like(outer_b)\n",
    "            mean0 += mean0_b / n\n",
    "            mean1 += mean1_b / n\n",
    "            std0 += std0_b / n\n",
    "            std1 += std1_b / n\n",
    "            outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1f9ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8b84a",
   "metadata": {},
   "source": [
    "# Find neuron-permutation for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "72842551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111 5911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 79.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 80.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 81.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 79.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 74.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 63.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 58.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 53.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 52.00it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 48.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 46.68it/s]\n"
     ]
    }
   ],
   "source": [
    "h = 4096\n",
    "dset = 'cifar10'\n",
    "layers = 10\n",
    "\n",
    "model0 = MLP(h, layers, dset).cuda()\n",
    "model1 = MLP(h, layers, dset).cuda()\n",
    "k0 = 'mlps/%s_e%d_l%d_h%d_v1' % (dset, 100, layers, h)\n",
    "k1 = 'mlps/%s_e%d_l%d_h%d_v2' % (dset, 100, layers, h)\n",
    "load_model(model0, k0)\n",
    "load_model(model1, k1)\n",
    "print(evaluate(model0), evaluate(model1))\n",
    "\n",
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model, k):\n",
    "        super().__init__()\n",
    "        self.grayscale = model.grayscale\n",
    "        self.fc1 = model.fc1\n",
    "        self.layers = model.layers[:2*k]\n",
    "    def forward(self, x):\n",
    "        if self.grayscale:\n",
    "            if x.size(1) == 3:\n",
    "                x = x.mean(1, keepdim=True)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "perm_map = get_layer_perm(Subnet(model0, 0), Subnet(model1, 0))\n",
    "fc = model1.fc1\n",
    "w_list = [fc.weight, fc.bias]\n",
    "for w in w_list:\n",
    "    w.data = w[perm_map]\n",
    "w = model1.layers[0].weight\n",
    "w.data = w.data[:, perm_map]\n",
    "\n",
    "########\n",
    "\n",
    "for k in range(1, 10+1):\n",
    "    perm_map = get_layer_perm(Subnet(model0, k), Subnet(model1, k))\n",
    "    fc = model1.layers[2*k-2]\n",
    "    w_list = [fc.weight, fc.bias]\n",
    "    for w in w_list:\n",
    "        w.data = w[perm_map]\n",
    "    if k < layers:\n",
    "        w = model1.layers[2*k].weight\n",
    "        w.data = w[:, perm_map]\n",
    "    else:\n",
    "        w = model1.fc2.weight\n",
    "        w.data = w[:, perm_map]\n",
    "\n",
    "save_model(model1, 'mlps/%s_e%d_l%d_h%d_v2_perm1' % (dset, 100, layers, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc494077",
   "metadata": {},
   "source": [
    "## Evaluate the interpolated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1a7ae6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(model, alpha, key0, key1):\n",
    "    sd0 = torch.load('%s.pt' % key0)\n",
    "    sd1 = torch.load('%s.pt' % key1)\n",
    "    sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "                for k in sd0.keys()}\n",
    "    model.load_state_dict(sd_alpha)\n",
    "\n",
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=2):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.modules.batchnorm._BatchNorm):\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in train_loader:\n",
    "                output = model(images.cuda())\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "39ee0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm1d(len(layer.weight))\n",
    "        self.rescale = False\n",
    "        \n",
    "    def set_stats(self, goal_mean, goal_var):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        goal_std = (goal_var + 1e-7).sqrt()\n",
    "        self.bn.weight.data = goal_std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x1 = self.bn(x)\n",
    "        return x1 if self.rescale else x\n",
    "\n",
    "def make_tracked_net(net):\n",
    "    net1 = MLP(layers=len(net.layers)//2, h=net.fc1.out_features, dset=net.dset)\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    net1.fc1 = ResetLayer(net1.fc1)\n",
    "    for i, layer in enumerate(net.layers):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            net1.layers[i] = ResetLayer(layer)\n",
    "#     net1.fc2 = ResetLayer(net1.fc2)\n",
    "    return net1.cuda().eval()\n",
    "\n",
    "def forward_pass_correction(wrap_a, model0, model1, alpha=0.5):\n",
    "    ## calculate the statistics of every hidden unit in the endpoint networks\n",
    "    ## this is done practically using PyTorch BatchNorm2d layers.\n",
    "    wrap0 = make_tracked_net(model0)\n",
    "    wrap1 = make_tracked_net(model1)\n",
    "    reset_bn_stats(wrap0)\n",
    "    reset_bn_stats(wrap1)\n",
    "    \n",
    "    ## set the goal mean/std in added bns of interpolated network, and turn batch renormalization on\n",
    "    for m_a, m0, m1 in zip(wrap_a.modules(), wrap0.modules(), wrap1.modules()):\n",
    "        if not isinstance(m0, ResetLayer):\n",
    "            continue\n",
    "        # get goal statistics -- interpolate the mean and std of parent networks\n",
    "        mu0 = m0.bn.running_mean\n",
    "        mu1 = m1.bn.running_mean\n",
    "        goal_mean = (1 - alpha) * mu0 + alpha * mu1\n",
    "        var0 = m0.bn.running_var\n",
    "        var1 = m1.bn.running_var\n",
    "        goal_var = ((1 - alpha) * var0.sqrt() + alpha * var1.sqrt()).square()\n",
    "        # set these in the interpolated bn controller\n",
    "        m_a.set_stats(goal_mean, goal_var)\n",
    "        # turn rescaling on\n",
    "        m_a.rescale = True\n",
    "    reset_bn_stats(wrap_a, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "27c9c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8b574dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:22<00:00,  7.48s/it]\n"
     ]
    }
   ],
   "source": [
    "dset = 'fashionmnist'\n",
    "layers = 10\n",
    "\n",
    "for h in tqdm([32, 64, 128, 256, 512, 1024, 2048, 4096][-3:]):\n",
    "    s = {}\n",
    "\n",
    "    k0 = 'mlps/%s_e%d_l%d_h%d_v1' % (dset, 100, layers, h)\n",
    "    k1 = 'mlps/%s_e%d_l%d_h%d_v2' % (dset, 100, layers, h)\n",
    "    model0 = MLP(h, layers, dset).cuda()\n",
    "    model1 = MLP(h, layers, dset).cuda()\n",
    "    load_model(model0, k0)\n",
    "    load_model(model1, k1)\n",
    "    s['v1'] = full_eval(model0)\n",
    "    s['v2'] = full_eval(model1)\n",
    "\n",
    "    model_a = MLP(h, layers, dset).cuda()\n",
    "    mix_weights(model_a, 0.5, k0, k1)\n",
    "    s['vanilla'] = full_eval(model_a)\n",
    "    \n",
    "    k1 = 'mlps/%s_e%d_l%d_h%d_v2_perm1' % (dset, 100, layers, h)\n",
    "    model_a = MLP(h, layers, dset).cuda()\n",
    "    mix_weights(model_a, 0.5, k0, k1)\n",
    "    s['permute'] = full_eval(model_a)\n",
    "\n",
    "    wrap_a = make_tracked_net(model_a)\n",
    "    forward_pass_correction(wrap_a, model0, model1, 0.5)\n",
    "    s['permute_correct'] = full_eval(wrap_a)\n",
    "    \n",
    "    stats['%s_e%d_l%d_h%d' % (dset, 100, layers, h)] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "cdebfe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stats, 'figure_objects/mlp_barriers_datasets.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc7317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc431b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afff45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
