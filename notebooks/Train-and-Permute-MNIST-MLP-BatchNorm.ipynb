{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e5ba46",
   "metadata": {},
   "source": [
    "# Train-and-Permute-MNIST-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0db779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e738d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1a55e",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c477d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./mlps3', exist_ok=True)\n",
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), 'mlps3/%s.pt' % i)\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load('mlps3/%s.pt' % i)\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e2e9f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write ffcv files (only needs to be run once)\n",
    "import torchvision\n",
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.writer import DatasetWriter\n",
    "\n",
    "transform = lambda img: img.convert('RGB')\n",
    "train_dset = torchvision.datasets.MNIST(root='/tmp', download=True, train=True, transform=transform)\n",
    "test_dset = torchvision.datasets.MNIST(root='/tmp', download=True, train=False, transform=transform)\n",
    "\n",
    "datasets = { \n",
    "    'train': train_dset,\n",
    "    'test': test_dset,\n",
    "}\n",
    "\n",
    "for (name, ds) in datasets.items():\n",
    "    writer = DatasetWriter(f'/tmp/mnist_{name}.beton', {\n",
    "        'image': RGBImageField(),\n",
    "        'label': IntField()\n",
    "    })  \n",
    "    writer.from_indexed_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d371c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "\n",
    "MNIST_MEAN = [33.32]\n",
    "MNIST_STD = [78.58]\n",
    "normalize = T.Normalize(np.array(MNIST_MEAN), np.array(MNIST_STD))\n",
    "\n",
    "## fast FFCV data loaders\n",
    "device = 'cuda:0' \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "pre_p = [SimpleRGBImageDecoder()]\n",
    "post_p = [\n",
    "    ToTensor(),\n",
    "    ToDevice(device, non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float16),\n",
    "    normalize,\n",
    "]\n",
    "train_loader = train_noaug_loader = Loader(f'/tmp/mnist_train.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.RANDOM,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})\n",
    "test_loader = Loader(f'/tmp/mnist_test.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e22780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct\n",
    "\n",
    "# evaluates acc and loss\n",
    "def evaluate2(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "            total += len(labels)\n",
    "            loss = F.cross_entropy(outputs, labels.cuda())\n",
    "            losses.append(loss.item())\n",
    "    return correct / total, np.array(losses).mean()\n",
    "\n",
    "def full_eval1(model):\n",
    "    tr_acc, tr_loss = evaluate2(model, loader=train_noaug_loader)\n",
    "    te_acc, te_loss = evaluate2(model, loader=test_loader)\n",
    "    return '%.2f, %.3f, %.2f, %.3f' % (100*tr_acc, tr_loss, 100*te_acc, te_loss)\n",
    "\n",
    "def full_eval(model):\n",
    "    tr_acc, tr_loss = evaluate2(model, loader=train_noaug_loader)\n",
    "    te_acc, te_loss = evaluate2(model, loader=test_loader)\n",
    "    return (100*tr_acc, tr_loss, 100*te_acc, te_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122fde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NormMLP(nn.Module):\n",
    "    def __init__(self, h=128, layers=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, h, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(h)\n",
    "        mid_layers = []\n",
    "        for _ in range(layers):\n",
    "            mid_layers.extend([\n",
    "                nn.Linear(h, h, bias=False),\n",
    "                nn.BatchNorm1d(h),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self.layers = nn.Sequential(*mid_layers)\n",
    "        self.fc2 = nn.Linear(h, 10)\n",
    "    def forward(self, x):\n",
    "        if x.size(1) == 3:\n",
    "            x = x.mean(1, keepdim=True)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.layers(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cdbf3",
   "metadata": {},
   "source": [
    "## Train and save two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be81b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(save_key, layers=5, h=512):\n",
    "    model = NormMLP(h=h, layers=layers).cuda()\n",
    "\n",
    "    optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "\n",
    "    EPOCHS = 50\n",
    "    ne_iters = len(train_loader)\n",
    "    lr_schedule = np.interp(np.arange(1+EPOCHS*ne_iters), [0, 5*ne_iters, EPOCHS*ne_iters], [0, 1, 0])\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_schedule.__getitem__)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    for _ in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                outputs = model(inputs.cuda())\n",
    "                loss = loss_fn(outputs, labels.cuda())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "    \n",
    "    print(evaluate(model))\n",
    "    save_model(model, save_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1a1226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:21<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:20<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for layers in range(8):\n",
    "for layers in [12]:\n",
    "    h = 128\n",
    "    train('mlp_e50_l%d_h%d_v1' % (layers, h), layers=layers, h=h)\n",
    "    train('mlp_e50_l%d_h%d_v2' % (layers, h), layers=layers, h=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e46b7b",
   "metadata": {},
   "source": [
    "### matching code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363e68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given two networks net0, net1 which each output a feature map of shape NxCxWxH\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the outputs of the two networks\n",
    "def run_corr_matrix(net0, net1, epochs=1, loader=train_loader):\n",
    "    n = epochs*len(loader)\n",
    "    mean0 = mean1 = std0 = std1 = None\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for _ in range(epochs):\n",
    "            for i, (images, _) in enumerate(tqdm(loader)):\n",
    "                img_t = images.float().cuda()\n",
    "                out0 = net0(img_t)\n",
    "                out0 = out0.reshape(out0.shape[0], out0.shape[1], -1).permute(0, 2, 1)\n",
    "                out0 = out0.reshape(-1, out0.shape[2]).double()\n",
    "\n",
    "                out1 = net1(img_t)\n",
    "                out1 = out1.reshape(out1.shape[0], out1.shape[1], -1).permute(0, 2, 1)\n",
    "                out1 = out1.reshape(-1, out1.shape[2]).double()\n",
    "\n",
    "                mean0_b = out0.mean(dim=0)\n",
    "                mean1_b = out1.mean(dim=0)\n",
    "                std0_b = out0.std(dim=0)\n",
    "                std1_b = out1.std(dim=0)\n",
    "                outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "\n",
    "                if i == 0:\n",
    "                    mean0 = torch.zeros_like(mean0_b)\n",
    "                    mean1 = torch.zeros_like(mean1_b)\n",
    "                    std0 = torch.zeros_like(std0_b)\n",
    "                    std1 = torch.zeros_like(std1_b)\n",
    "                    outer = torch.zeros_like(outer_b)\n",
    "                mean0 += mean0_b / n\n",
    "                mean1 += mean1_b / n\n",
    "                std0 += std0_b / n\n",
    "                std1 += std1_b / n\n",
    "                outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f9ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8b84a",
   "metadata": {},
   "source": [
    "# Find neuron-permutation for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72842551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9817 9827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 252.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 253.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 277.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 283.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 352.81it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 377.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 379.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 327.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 315.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 334.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 307.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 307.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 310.28it/s]\n"
     ]
    }
   ],
   "source": [
    "v1, v2 = 1, 2\n",
    "h = 128\n",
    "layers = 12\n",
    "\n",
    "model0 = NormMLP(h=h, layers=layers).cuda()\n",
    "model1 = NormMLP(h=h, layers=layers).cuda()\n",
    "load_model(model0, 'mlp_e50_l%d_h%d_v%d' % (layers, h, v1))\n",
    "load_model(model1, 'mlp_e50_l%d_h%d_v%d'% (layers, h, v2))\n",
    "print(evaluate(model0), evaluate(model1))\n",
    "\n",
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model, layer_i):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer_i = layer_i\n",
    "    def forward(self, x):\n",
    "        if x.size(1) == 3:\n",
    "            x = x.mean(1, keepdim=True)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.model.fc1(x))\n",
    "        x = self.model.layers[:3*self.layer_i](x)\n",
    "        return x\n",
    "\n",
    "perm_map = get_layer_perm(Subnet(model0, layer_i=0), Subnet(model1, layer_i=0))\n",
    "fc = model1.fc1\n",
    "w_list = [fc.weight]\n",
    "bn = model1.bn1\n",
    "w_list.extend([bn.weight,\n",
    "                bn.bias,\n",
    "                bn.running_mean,\n",
    "                bn.running_var])\n",
    "for w in w_list:\n",
    "    w.data = w[perm_map]\n",
    "for w in [model1.layers[0].weight]:\n",
    "    w.data = w.data[:, perm_map]\n",
    "\n",
    "########\n",
    "\n",
    "for i in range(layers):\n",
    "    perm_map = get_layer_perm(Subnet(model0, layer_i=i+1), Subnet(model1, layer_i=i+1))\n",
    "    fc = model1.layers[3*i]\n",
    "    w_list = [fc.weight]\n",
    "    bn = model1.layers[3*i+1]\n",
    "    w_list.extend([bn.weight,\n",
    "                    bn.bias,\n",
    "                    bn.running_mean,\n",
    "                    bn.running_var])\n",
    "    for w in w_list:\n",
    "        w.data = w[perm_map]\n",
    "    if i < layers-1:\n",
    "        for w in [model1.layers[3*(i+1)].weight]:\n",
    "            w.data = w[:, perm_map]\n",
    "w = model1.fc2.weight\n",
    "w.data = w[:, perm_map]\n",
    "\n",
    "save_model(model1, 'mlp_e50_l%d_h%d_v%d_perm%d' % (layers, h, v2, v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc494077",
   "metadata": {},
   "source": [
    "## Evaluate the interpolated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7ae6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(model, alpha, key0, key1, prefuse=False, premodel=None):\n",
    "    sd0 = torch.load('mlps3/%s.pt' % key0)\n",
    "    sd1 = torch.load('mlps3/%s.pt' % key1)\n",
    "    if prefuse:\n",
    "        premodel.load_state_dict(sd0)\n",
    "        sd0 = fuse_mlp(premodel).state_dict()\n",
    "        premodel.load_state_dict(sd1)\n",
    "        sd1 = fuse_mlp(premodel).state_dict()\n",
    "    sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "                for k in sd0.keys()}\n",
    "    model.load_state_dict(sd_alpha)\n",
    "\n",
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bf7c1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0.0) (100.0, 0.0001097355286522846, 98.32, 0.08815740388818086)\n",
      "(α=1.0) (100.0, 0.00010460326666361652, 98.36, 0.0902980868704617)\n",
      "(α=0.5 permuted) (87.33, 0.44000959893067676, 87.47, 0.4413963109254837)\n",
      "(α=0.5 permuted+corrected) (95.90666666666667, 0.1948693387210369, 95.44, 0.23052519261837007)\n"
     ]
    }
   ],
   "source": [
    "layers = 9\n",
    "pre = 'mlp_e50_l%d_h%d' % (layers, h)\n",
    "v1, v2 = 1, 2\n",
    "\n",
    "model0 = NormMLP(h=h, layers=layers).cuda()\n",
    "model1 = NormMLP(h=h, layers=layers).cuda()\n",
    "model_a = NormMLP(h=h, layers=layers).cuda()\n",
    "load_model(model0, '%s_v%d' % (pre, v1))\n",
    "load_model(model1, '%s_v%d_perm%d' % (pre, v2, v1))\n",
    "mix_weights(model_a, 0.5, '%s_v%d' % (pre, v1), '%s_v%d_perm%d' % (pre, v2, v1))\n",
    "\n",
    "print('(α=0.0)', full_eval(model0))\n",
    "print('(α=1.0)', full_eval(model1))\n",
    "print('(α=0.5 permuted)', full_eval(model_a))\n",
    "reset_bn_stats(model_a)\n",
    "print('(α=0.5 permuted+corrected)', full_eval(model_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5dff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c4e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
