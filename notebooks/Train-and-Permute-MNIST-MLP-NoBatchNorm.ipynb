{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70721b0d",
   "metadata": {},
   "source": [
    "# Train-and-Permute-MNIST-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0db779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e738d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1a55e",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c477d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./mlps2', exist_ok=True)\n",
    "def save_model(model, i):\n",
    "    sd = model.state_dict()\n",
    "    torch.save(model.state_dict(), 'mlps2/%s.pt' % i)\n",
    "\n",
    "def load_model(model, i):\n",
    "    sd = torch.load('mlps2/%s.pt' % i)\n",
    "    model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e2e9f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## write ffcv files (only needs to be run once)\n",
    "# import torchvision\n",
    "# from ffcv.fields import IntField, RGBImageField\n",
    "# from ffcv.writer import DatasetWriter\n",
    "\n",
    "# transform = lambda img: img.convert('RGB')\n",
    "# train_dset = torchvision.datasets.MNIST(root='/tmp', download=True, train=True, transform=transform)\n",
    "# test_dset = torchvision.datasets.MNIST(root='/tmp', download=True, train=False, transform=transform)\n",
    "\n",
    "# datasets = { \n",
    "#     'train': train_dset,\n",
    "#     'test': test_dset,\n",
    "# }\n",
    "\n",
    "# for (name, ds) in datasets.items():\n",
    "#     writer = DatasetWriter(f'/tmp/mnist_{name}.beton', {\n",
    "#         'image': RGBImageField(),\n",
    "#         'label': IntField()\n",
    "#     })  \n",
    "#     writer.from_indexed_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d371c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffcv.fields import IntField, RGBImageField\n",
    "from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "from ffcv.loader import Loader, OrderOption\n",
    "from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "    RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "from ffcv.transforms.common import Squeeze\n",
    "\n",
    "MNIST_MEAN = [33.32]\n",
    "MNIST_STD = [78.58]\n",
    "normalize = T.Normalize(np.array(MNIST_MEAN), np.array(MNIST_STD))\n",
    "\n",
    "## fast FFCV data loaders\n",
    "device = 'cuda:0' \n",
    "label_pipeline = [IntDecoder(), ToTensor(), ToDevice(device), Squeeze()]\n",
    "pre_p = [SimpleRGBImageDecoder()]\n",
    "post_p = [\n",
    "    ToTensor(),\n",
    "    ToDevice(device, non_blocking=True),\n",
    "    ToTorchImage(),\n",
    "    Convert(torch.float16),\n",
    "    normalize,\n",
    "]\n",
    "train_loader = train_noaug_loader = Loader(f'/tmp/mnist_train.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.RANDOM,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})\n",
    "test_loader = Loader(f'/tmp/mnist_test.beton',\n",
    "                     batch_size=1000,\n",
    "                     num_workers=8,\n",
    "                     order=OrderOption.SEQUENTIAL,\n",
    "                     drop_last=False,\n",
    "                     pipelines={'image': pre_p+post_p,\n",
    "                                'label': label_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e22780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates accuracy\n",
    "def evaluate(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "    return correct\n",
    "\n",
    "# evaluates acc and loss\n",
    "def evaluate2(model, loader=test_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(), autocast():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs.cuda())\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (labels.cuda() == pred).sum().item()\n",
    "            total += len(labels)\n",
    "            loss = F.cross_entropy(outputs, labels.cuda())\n",
    "            losses.append(loss.item())\n",
    "    return correct / total, np.array(losses).mean()\n",
    "\n",
    "def full_eval1(model):\n",
    "    tr_acc, tr_loss = evaluate2(model, loader=train_noaug_loader)\n",
    "    te_acc, te_loss = evaluate2(model, loader=test_loader)\n",
    "    return '%.2f, %.3f, %.2f, %.3f' % (100*tr_acc, tr_loss, 100*te_acc, te_loss)\n",
    "\n",
    "def full_eval(model):\n",
    "    tr_acc, tr_loss = evaluate2(model, loader=train_noaug_loader)\n",
    "    te_acc, te_loss = evaluate2(model, loader=test_loader)\n",
    "    return (100*tr_acc, tr_loss, 100*te_acc, te_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122fde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, h=128, layers=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, h, bias=True)\n",
    "        mid_layers = []\n",
    "        for _ in range(layers):\n",
    "            mid_layers.extend([\n",
    "                nn.Linear(h, h, bias=True),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self.layers = nn.Sequential(*mid_layers)\n",
    "        self.fc2 = nn.Linear(h, 10)\n",
    "    def forward(self, x):\n",
    "        if x.size(1) == 3:\n",
    "            x = x.mean(1, keepdim=True)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.layers(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cdbf3",
   "metadata": {},
   "source": [
    "## Train and save two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4abe22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(save_key, layers=5, h=512):\n",
    "    model = MLP(h=h, layers=layers).cuda()\n",
    "\n",
    "    optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "\n",
    "    EPOCHS = 50\n",
    "    ne_iters = len(train_loader)\n",
    "    lr_schedule = np.interp(np.arange(1+EPOCHS*ne_iters), [0, 5*ne_iters, EPOCHS*ne_iters], [0, 1, 0])\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_schedule.__getitem__)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    for _ in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast():\n",
    "                outputs = model(inputs.cuda())\n",
    "                loss = loss_fn(outputs, labels.cuda())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "    \n",
    "    print(evaluate(model))\n",
    "    save_model(model, save_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd1a1226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:23<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:21<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for layers in [6]:\n",
    "    h = 128\n",
    "    train('mlp_e50_l%d_h%d_v1' % (layers, h), layers=layers, h=h)\n",
    "    train('mlp_e50_l%d_h%d_v2' % (layers, h), layers=layers, h=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e46b7b",
   "metadata": {},
   "source": [
    "### matching code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363e68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given two networks net0, net1 which each output a feature map of shape NxCxWxH\n",
    "# this will reshape both outputs to (N*W*H)xC\n",
    "# and then compute a CxC correlation matrix between the outputs of the two networks\n",
    "def run_corr_matrix(net0, net1, epochs=1, loader=train_loader):\n",
    "    n = epochs*len(loader)\n",
    "    mean0 = mean1 = std0 = std1 = None\n",
    "    with torch.no_grad():\n",
    "        net0.eval()\n",
    "        net1.eval()\n",
    "        for _ in range(epochs):\n",
    "            for i, (images, _) in enumerate(tqdm(loader)):\n",
    "                img_t = images.float().cuda()\n",
    "                out0 = net0(img_t)\n",
    "                out0 = out0.reshape(out0.shape[0], out0.shape[1], -1).permute(0, 2, 1)\n",
    "                out0 = out0.reshape(-1, out0.shape[2]).double()\n",
    "\n",
    "                out1 = net1(img_t)\n",
    "                out1 = out1.reshape(out1.shape[0], out1.shape[1], -1).permute(0, 2, 1)\n",
    "                out1 = out1.reshape(-1, out1.shape[2]).double()\n",
    "\n",
    "                mean0_b = out0.mean(dim=0)\n",
    "                mean1_b = out1.mean(dim=0)\n",
    "                std0_b = out0.std(dim=0)\n",
    "                std1_b = out1.std(dim=0)\n",
    "                outer_b = (out0.T @ out1) / out0.shape[0]\n",
    "\n",
    "                if i == 0:\n",
    "                    mean0 = torch.zeros_like(mean0_b)\n",
    "                    mean1 = torch.zeros_like(mean1_b)\n",
    "                    std0 = torch.zeros_like(std0_b)\n",
    "                    std1 = torch.zeros_like(std1_b)\n",
    "                    outer = torch.zeros_like(outer_b)\n",
    "                mean0 += mean0_b / n\n",
    "                mean1 += mean1_b / n\n",
    "                std0 += std0_b / n\n",
    "                std1 += std1_b / n\n",
    "                outer += outer_b / n\n",
    "\n",
    "    cov = outer - torch.outer(mean0, mean1)\n",
    "    corr = cov / (torch.outer(std0, std1) + 1e-4)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f9ef263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_perm1(corr_mtx):\n",
    "    corr_mtx_a = corr_mtx.cpu().numpy()\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(corr_mtx_a, maximize=True)\n",
    "    assert (row_ind == np.arange(len(corr_mtx_a))).all()\n",
    "    perm_map = torch.tensor(col_ind).long()\n",
    "    return perm_map\n",
    "\n",
    "# returns the channel-permutation to make layer1's activations most closely\n",
    "# match layer0's.\n",
    "def get_layer_perm(net0, net1):\n",
    "    corr_mtx = run_corr_matrix(net0, net1)\n",
    "    return get_layer_perm1(corr_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8b84a",
   "metadata": {},
   "source": [
    "# Find neuron-permutation for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72842551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9651 9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 285.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 271.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 254.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 248.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 386.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 335.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 397.09it/s]\n"
     ]
    }
   ],
   "source": [
    "v1, v2 = 1, 2\n",
    "h = 128\n",
    "layers = 6\n",
    "\n",
    "model0 = MLP(h=h, layers=layers).cuda()\n",
    "model1 = MLP(h=h, layers=layers).cuda()\n",
    "load_model(model0, 'mlp_e50_l%d_h%d_v%d' % (layers, h, v1))\n",
    "load_model(model1, 'mlp_e50_l%d_h%d_v%d'% (layers, h, v2))\n",
    "print(evaluate(model0), evaluate(model1))\n",
    "\n",
    "class Subnet(nn.Module):\n",
    "    def __init__(self, model, layer_i):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layer_i = layer_i\n",
    "    def forward(self, x):\n",
    "        if x.size(1) == 3:\n",
    "            x = x.mean(1, keepdim=True)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.model.fc1(x))\n",
    "        x = self.model.layers[:2*self.layer_i](x)\n",
    "        return x\n",
    "\n",
    "perm_map = get_layer_perm(Subnet(model0, layer_i=0), Subnet(model1, layer_i=0))\n",
    "fc = model1.fc1\n",
    "w_list = [fc.weight, fc.bias]\n",
    "for w in w_list:\n",
    "    w.data = w[perm_map]\n",
    "for w in [model1.layers[0].weight]:\n",
    "    w.data = w.data[:, perm_map]\n",
    "\n",
    "########\n",
    "\n",
    "for i in range(layers):\n",
    "    perm_map = get_layer_perm(Subnet(model0, layer_i=i+1), Subnet(model1, layer_i=i+1))\n",
    "    fc = model1.layers[2*i]\n",
    "    w_list = [fc.weight, fc.bias]\n",
    "    for w in w_list:\n",
    "        w.data = w[perm_map]\n",
    "    if i < layers-1:\n",
    "        for w in [model1.layers[2*i+2].weight]:\n",
    "            w.data = w[:, perm_map]\n",
    "w = model1.fc2.weight\n",
    "w.data = w[:, perm_map]\n",
    "\n",
    "save_model(model1, 'mlp_e50_l%d_h%d_v%d_perm%d' % (layers, h, v2, v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc494077",
   "metadata": {},
   "source": [
    "## Evaluate the interpolated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7ae6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_weights(model, alpha, key0, key1, prefuse=False, premodel=None):\n",
    "    sd0 = torch.load('mlps2/%s.pt' % key0)\n",
    "    sd1 = torch.load('mlps2/%s.pt' % key1)\n",
    "    if prefuse:\n",
    "        premodel.load_state_dict(sd0)\n",
    "        sd0 = fuse_mlp(premodel).state_dict()\n",
    "        premodel.load_state_dict(sd1)\n",
    "        sd1 = fuse_mlp(premodel).state_dict()\n",
    "    sd_alpha = {k: (1 - alpha) * sd0[k].cuda() + alpha * sd1[k].cuda()\n",
    "                for k in sd0.keys()}\n",
    "    model.load_state_dict(sd_alpha)\n",
    "\n",
    "# use the train loader with data augmentation as this gives better results\n",
    "def reset_bn_stats(model, epochs=1, loader=train_loader):\n",
    "    # resetting stats to baseline first as below is necessary for stability\n",
    "    for m in model.modules():\n",
    "        if type(m) == nn.BatchNorm2d:\n",
    "            m.momentum = None # use simple average\n",
    "            m.reset_running_stats()\n",
    "    # run a single train epoch with augmentations to recalc stats\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        with torch.no_grad(), autocast():\n",
    "            for images, _ in loader:\n",
    "                output = model(images.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2814f9",
   "metadata": {},
   "source": [
    "## Evaluate interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7e6d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetLinear(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.h = h = layer.out_features if hasattr(layer, 'out_features') else 768\n",
    "        self.layer = layer\n",
    "        self.bn = nn.BatchNorm1d(h)\n",
    "        self.rescale = False\n",
    "        \n",
    "    def set_stats(self, goal_mean, goal_var):\n",
    "        self.bn.bias.data = goal_mean\n",
    "        goal_std = (goal_var + 1e-5).sqrt()\n",
    "        self.bn.weight.data = goal_std\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        x = self.layer(*args, **kwargs)\n",
    "        if self.rescale:\n",
    "            x = self.bn(x)\n",
    "        else:\n",
    "            self.bn(x)\n",
    "        return x\n",
    "    \n",
    "def make_tracked_net(net):\n",
    "    net1 = MLP(layers=len(net.layers)//2, h=net.fc1.out_features).cuda()\n",
    "    net1.load_state_dict(net.state_dict())\n",
    "    net1.fc1 = ResetLinear(net1.fc1)\n",
    "    for i in range(len(net1.layers)):\n",
    "        if isinstance(net1.layers[i], nn.Linear):\n",
    "            net1.layers[i] = ResetLinear(net1.layers[i])\n",
    "    net1.fc2 = ResetLinear(net1.fc2)\n",
    "    return net1.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9127f4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(α=0.0) (99.91499999999999, 0.004833848240862911, 96.50999999999999, 0.2323371425271034)\n",
      "(α=0.5 permuted) (72.05666666666667, 0.9436834822098414, 71.67999999999999, 0.9676996171474457)\n",
      "(α=1.0) (99.84666666666666, 0.0088561528439944, 96.59, 0.21037664376199244)\n",
      "(α=0.5 permuted+corrected) (83.71166666666666, 0.7953927487134933, 82.96, 0.8877289235591889)\n"
     ]
    }
   ],
   "source": [
    "layers = 6\n",
    "pre = 'mlp_e50_l%d_h%d' % (layers, h)\n",
    "v1, v2 = 1, 2\n",
    "\n",
    "model0 = MLP(h=h, layers=layers).cuda()\n",
    "model1 = MLP(h=h, layers=layers).cuda()\n",
    "model_a = MLP(h=h, layers=layers).cuda()\n",
    "load_model(model0, '%s_v%d' % (pre, v1))\n",
    "load_model(model1, '%s_v%d_perm%d' % (pre, v2, v1))\n",
    "mix_weights(model_a, 0.5, '%s_v%d' % (pre, v1), '%s_v%d_perm%d' % (pre, v2, v1))\n",
    "print('(α=0.0)', full_eval(model0))\n",
    "print('(α=0.5 permuted)', full_eval(model_a))\n",
    "print('(α=1.0)', full_eval(model1))\n",
    "\n",
    "\n",
    "## calculate the statistics of every hidden unit in the endpoint networks\n",
    "## this is done practically using PyTorch BatchNorm2d layers.\n",
    "wrap0 = make_tracked_net(model0)\n",
    "wrap1 = make_tracked_net(model1)\n",
    "reset_bn_stats(wrap0)\n",
    "reset_bn_stats(wrap1)\n",
    "\n",
    "wrap_a = make_tracked_net(model_a)\n",
    "## set the goal mean/std in added bns of interpolated network, and turn batch renormalization on\n",
    "for m0, m_a, m1 in zip(wrap0.modules(), wrap_a.modules(), wrap1.modules()):\n",
    "    if not isinstance(m0, ResetLinear):\n",
    "        continue\n",
    "    # get goal statistics -- interpolate the mean and std of parent networks\n",
    "    mu0 = m0.bn.running_mean\n",
    "    mu1 = m1.bn.running_mean\n",
    "    goal_mean = (mu0 + mu1)/2\n",
    "    var0 = m0.bn.running_var\n",
    "    var1 = m1.bn.running_var\n",
    "    goal_var = ((var0.sqrt() + var1.sqrt())/2).square()\n",
    "    # set these in the interpolated bn controller\n",
    "    m_a.set_stats(goal_mean, goal_var)\n",
    "    # turn rescaling on\n",
    "    m_a.rescale = True\n",
    "    \n",
    "# reset the tracked mean/var and fuse rescalings back into conv layers \n",
    "reset_bn_stats(wrap_a)\n",
    "# # fuse the rescaling+shift coefficients back into conv layers\n",
    "# model_b = fuse_tracked_net(wrap_a, w)\n",
    "print('(α=0.5 permuted+corrected)', full_eval(wrap_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df107b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c4e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
